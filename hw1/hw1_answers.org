* Problem 1.1: Two Datasets
** Dolma
*** Knowledge cutoff
Depending on source varies from Apr 2019 to Oct 2023. 
*** Data source
Mix of web content, academic publications, code, books and encyclopedic materials.
*** Model trained on the dataset
OPT
*** Dataset license
ODC-By. Different parts of dataset has different copyright protections. E.g., Arxiv (academic publications, most published under Creative Commons) vs Common crawl (web crawl, variable and/or unclear/unspecified copyrighting)
*** Task where trained model will do poorly
1. Tasks requiring up-to-date information.
2. Highly technical or scientific domains.
3. Reasoning tasks.

** Project Gutenberg
*** Knowledge cutoff
Unclear, whatever was the publication date of the latest book. Most of the content would be decades old.
*** Data source
Uncopyrighted (in the US) books and books explicitly allowed by authors for use by the project.
*** Model trained on the dataset
It is part of Dolma. Hence any models trained on Dolma. It is probably a part of many other datasets as well.
*** Dataset license
Uncopyrighted work can be distributed freely by anyone. For works where authors grant permission to the project, redistribution is restricted. Does LLMs output count as a redistribution? Debatable.
*** Task where trained model will do poorly
1. Generating a blog post. The tone of blogs is very different from that of most books.

* Problem 1.2: Three Models
** Three models chosen
GPT-2, LLaMA 2, OPT
** Data training and processing
*** GPT2
**** Dataset
They scraped all the web pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released.
**** Processing
BPE with 50257 vocab size. Batch size of 1024 tokens.
*** LLaMA 2
**** Dataset
Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.
**** Processing
Could find no information on this.
*** OPT
**** Dataset
Consists of BookCorpus, CC-Stories, The Pile, Pushshift.io, CCNewsV2
**** Processing
GPT-2 BPE, batch size of 2048 tokens.
** A use case for LLM trained on publicly accessible data
1. Publicly available datasets are often free or low-cost.
2. Research uses, such as ablation studies (for outcomes such as bias etc.)
* Problem 2.1
** Number of HTML pages
6368
** Code and inline HTML
It parses it as is, retaining the white space formatting.
** How does it handle HTML tags
Pretty much removes all formatting tags (headings, tables, paragraphs etc). Ignores images. 
** WET vs Cleaned HTML
The most significant differences I see is that
1. My html_to_text filters out non-roman alphabet languages.
2. html_to_text has very permissive puntuation set. Therefore a lot of inline characters like ##, * etc make it into text.
 WET version is probably better because of more restrictive punctuation set. It may be better for multi-lingual training as well. But that depends on the use case.
* Problem 2.2
** Documents Deleted
2572, 40% considered low quality
** Low quality docs that passed the filter
1. http://18ha.e11.tw/tag/869
   It is mostly just URLs. Maybe lines that are just URLs must be filtered out.
2. http://101lab.net/blog/2004/06/post-276.html
   This is mostly chinese characters intersperced with dates and []. Maybe have a more restrictive punctuation set, fiter out links and have some assertion on distribution of alphabets and numbers in a paragraph.
** Non-english languages
My filter tries to exclude all texts not in roman script.
** Domain specific filtering
Coding domain will have different cleaning and fitering requirements. Depending on use case, we may want to remove comments. Filtering will certainly involve coinsider files names with certain extensions only.
** Additional Data Filtering Stages
1. Language filtering (natural or coding)
2. Classifier based quality filtering
3. Deduplication (approximate or exact matching)
4. Domain specific cleaning, possibly based on different word distributions.
* Problem 2.3
1. How long in seconds does it take to load the dataset?
   Around 270 seconds. Processing around 25 documents per second. For 3 billion documents this should take around 10^8 seconds or around 3 years.
2. How to make it faster.
   2.1 Parallelize in one machine using threads
   2.2 Parallelize across machiens using Flume like frameworks
   2.3 Minimize processing done in Python. Instead use libraries where the core functionality is implemented in C and that can process entire document as a unit.
3. One advantage of using packing over padding is to minimize waste of training iterations. This is especially relevant for large models.
