* Problem 1.1: Two Datasets
** Dolma
*** Knowledge cutoff
Depending on source varies from Apr 2019 to Oct 2023. 
*** Data source
Mix of web content, academic publications, code, books and encyclopedic materials.
*** Model trained on the dataset
OPT
*** Dataset license
ODC-By. Different parts of dataset has different copyright protections. E.g., Arxiv (academic publications, most published under Creative Commons) vs Common crawl (web crawl, variable and/or unclear/unspecified copyrighting)
*** Task where trained model will do poorly
1. Tasks requiring up-to-date information.
2. Highly technical or scientific domains.
3. Reasoning tasks.

** Project Gutenberg
*** Knowledge cutoff
Unclear, whatever was the publication date of the latest book. Most of the content would be decades old.
*** Data source
Uncopyrighted (in the US) books and books explicitly allowed by authors for use by the project.
*** Model trained on the dataset
It is part of Dolma. Hence any models trained on Dolma. It is probably a part of many other datasets as well.
*** Dataset license
Uncopyrighted work can be distributed freely by anyone. For works where authors grant permission to the project, redistribution is restricted. Does LLMs output count as a redistribution? Debatable.
*** Task where trained model will do poorly
1. Writing a blog post. The tone of blogs is very different from that of most books.

* Problem 1.2: Three Models
** Three models chosen
GPT-2, LLaMA 2, OPT
** Data training and processing
*** GPT2
**** Dataset
They scraped all the web pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released.
**** Processing
BPE with 50257 vocab size. Batch size of 1024 tokens.
*** LLaMA 2
**** Dataset
Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.
**** Processing
Could find no information on this.
*** OPT
**** Dataset
Consists of BookCorpus, CC-Stories, The Pile, Pushshift.io, CCNewsV2
**** Processing
GPT-2 BPE, batch size of 2048 tokens.
** A use case for LLM trained on publicly accessible data
1. Publicly available datasets are often free or low-cost.
2. Research uses, such as ablation studies (for outcomes such as bias etc.)
* Problem 2.1
** Number of HTML pages
6368
** Code and inline HTML
It parses it as is, retaining the white space formatting.
** How does it handle HTML tags
Pretty much removes all formatting tags (headings, tables, paragraphs etc). Ignores images. 
** WET vs Cleaned HTML
The most significant differences I see is that
1. html_to_text filters out non-roman alphabet languages.
2. html_to_text has very permissive puntuation set. Therefore a lot of inline characters like ##, * etc make it into text.
 WET version is probably better because of more restrictive punctuation set. It may be better for multi-lingual training as well. But that depends on the use case.
* Problem 2.2
** Documents Deleted
1914, 30% considered low quality
** Low quality docs that passed the filter
1. http://01tutorials.com/profile/inetryconydot/150/&sort=3
   It is just a menu of eclectic links. Mayve docs with mostly 1-4 word paragraphs should be filtered out.
2. http://18ha.e11.tw/tag/869
   This is mostly chinese characters but passed because of characters in links and lines with just space. Maybe filter out links from texts and discount lines with just space.
** Non-english languages
My filter tries to exclude all texts not in roman script.
