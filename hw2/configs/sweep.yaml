output_dir: outputs/final_sweep  # <- where the output files are written
method: random
num_run: 20                    # Not relevant if method is all
tokenizer_encoding: gpt2      # <- the tokenizer encoding, used by tiktoken (YOU SHOULD NOT CHANGE THIS)
save_model: False
device: auto                  # <- which device to put the model on (YOU DO NOT NEED TO CHANGE THIS)
name_prefix: final-sweep
tag: final-sweep
early_stopping: True
early_stopping_loss: 100
early_stopping_min_steps: 2000
num_warmup_steps: 100
max_flops: 1e+15
parameters:
  n_embd:
    - 64
    - 128                      # <- dimension of token and positional embeddings 
  n_head:
    - 4
    - 8                 # <- number of attention heads in multihead attention
  n_positions:
    - 64
    - 128           # <- the maximum number of tokens that the model can take
  n_layer:
    - 2
    - 4                # <- number of decoder blocks
  batch_size:
    - 32
    - 64                # <- number of sequences to feed into the model at a time
  lr:
    - [1e-5, 5e-5]
    - [2e-5, 1e-4]
    - [4e-5, 2e-4]
    - [5e-6, 2.5e-5]
    - [2.5e-6, 1.25e-5]
